{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hello/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#import sys\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from pprint import pprint\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "import json\n",
    "#from sklearn.decomposition import PCA, FastICA\n",
    "#n_comp=10\n",
    "from sklearn.utils import shuffle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the training data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Date and time</th>\n",
       "      <th>Query</th>\n",
       "      <th>Tweeter</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment     Unknown                 Date and time     Query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "           Tweeter                                              Tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=[\"Sentiment\",\"Unknown\",\"Date and time\",\"Query\",\"Tweeter\",\"Tweet\"]\n",
    "train=pd.read_csv(\"training.csv\", names=columns , encoding='latin-1')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value uptil 799999 are negative and rest are positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "train=train.drop([\"Unknown\",\"Date and time\",\"Query\",\"Tweeter\"],axis=1)\n",
    "print(train.iloc[799999]['Sentiment'])\n",
    "print(train.iloc[800000]['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                              Tweet\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Processing\n",
    "Using regular expressions to remove unnecessary expressiona and spaces. NLTK stemmer and stopwords to remove and merge certain words. The countvectorizer generates features based on the presence and absence of a word.\n",
    "### INCREASE VALUE OF \"count\" VARIABLE TO INCREASE TRAINING DATASET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_0</th>\n",
       "      <th>variable_1</th>\n",
       "      <th>variable_2</th>\n",
       "      <th>variable_3</th>\n",
       "      <th>variable_4</th>\n",
       "      <th>variable_5</th>\n",
       "      <th>variable_6</th>\n",
       "      <th>variable_7</th>\n",
       "      <th>variable_8</th>\n",
       "      <th>variable_9</th>\n",
       "      <th>...</th>\n",
       "      <th>variable_90</th>\n",
       "      <th>variable_91</th>\n",
       "      <th>variable_92</th>\n",
       "      <th>variable_93</th>\n",
       "      <th>variable_94</th>\n",
       "      <th>variable_95</th>\n",
       "      <th>variable_96</th>\n",
       "      <th>variable_97</th>\n",
       "      <th>variable_98</th>\n",
       "      <th>variable_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable_0  variable_1  variable_2  variable_3  variable_4  variable_5  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           0           0           0           0   \n",
       "2           0           0           0           0           0           0   \n",
       "3           0           0           0           0           0           0   \n",
       "4           0           0           0           0           0           0   \n",
       "\n",
       "   variable_6  variable_7  variable_8  variable_9     ...       variable_90  \\\n",
       "0           0           0           0           1     ...                 0   \n",
       "1           0           0           0           0     ...                 0   \n",
       "2           0           0           0           0     ...                 0   \n",
       "3           0           0           0           0     ...                 0   \n",
       "4           0           0           0           0     ...                 0   \n",
       "\n",
       "   variable_91  variable_92  variable_93  variable_94  variable_95  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   variable_96  variable_97  variable_98  variable_99  \n",
       "0            0            0            0            0  \n",
       "1            0            0            0            0  \n",
       "2            0            0            0            0  \n",
       "3            0            0            0            0  \n",
       "4            0            0            0            0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=50000\n",
    "\n",
    "kickdesc_negative = pd.Series(train.iloc[0:count,:]['Tweet'].tolist()).astype(str)\n",
    "kickdesc_positive=pd.Series(train.iloc[800000:800000+count,:]['Tweet'].tolist()).astype(str)\n",
    "# this function cleans punctuations, digits and irregular tabs. Then converts the sentences to lower\n",
    "def desc_clean(word):\n",
    "    p1 = re.sub(pattern='(\\W+)|(\\d+)|(\\s+)',repl=' ',string=word)\n",
    "    p1 = p1.lower()\n",
    "    return p1\n",
    "kickdesc_positive = kickdesc_positive.map(desc_clean)\n",
    "kickdesc_negative = kickdesc_negative.map(desc_clean)\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "kickdesc_positive = [[x for x in x.split() if x not in stop] for x in kickdesc_positive]\n",
    "kickdesc_negative = [[x for x in x.split() if x not in stop] for x in kickdesc_negative]\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "kickdesc_positive = [[stemmer.stem(x) for x in x] for x in kickdesc_positive]\n",
    "kickdesc_negative = [[stemmer.stem(x) for x in x] for x in kickdesc_negative]\n",
    "\n",
    "kickdesc_positive = [[x for x in x if len(x) > 2] for x in kickdesc_positive]\n",
    "kickdesc_negative = [[x for x in x if len(x) > 2] for x in kickdesc_negative]\n",
    "\n",
    "kickdesc_positive = [' '.join(x) for x in kickdesc_positive]\n",
    "kickdesc_negative = [' '.join(x) for x in kickdesc_negative]\n",
    "\n",
    "alldesc=kickdesc_negative+kickdesc_positive\n",
    "\n",
    "cv = CountVectorizer(max_features=100)\n",
    "\n",
    "#alldesc_positive = cv.fit_transform(kickdesc_positive).todense()\n",
    "#alldesc_negative = cv.fit_transform(kickdesc_negative).todense()\n",
    "alldesc = cv.fit_transform(alldesc).todense()\n",
    "combine = pd.DataFrame(alldesc)\n",
    "\n",
    "combine.rename(columns= lambda x: 'variable_'+ str(x), inplace=True)\n",
    "combine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling and converting dataset to binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>variable_0</th>\n",
       "      <th>variable_1</th>\n",
       "      <th>variable_2</th>\n",
       "      <th>variable_3</th>\n",
       "      <th>variable_4</th>\n",
       "      <th>variable_5</th>\n",
       "      <th>variable_6</th>\n",
       "      <th>variable_7</th>\n",
       "      <th>variable_8</th>\n",
       "      <th>...</th>\n",
       "      <th>variable_90</th>\n",
       "      <th>variable_91</th>\n",
       "      <th>variable_92</th>\n",
       "      <th>variable_93</th>\n",
       "      <th>variable_94</th>\n",
       "      <th>variable_95</th>\n",
       "      <th>variable_96</th>\n",
       "      <th>variable_97</th>\n",
       "      <th>variable_98</th>\n",
       "      <th>variable_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38735</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50451</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98518</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59451</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29756</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment  variable_0  variable_1  variable_2  variable_3  variable_4  \\\n",
       "38735          0           0           0           0           0           0   \n",
       "50451          1           0           0           0           0           0   \n",
       "98518          1           0           0           0           0           0   \n",
       "59451          1           0           0           0           0           0   \n",
       "29756          0           0           0           0           0           0   \n",
       "\n",
       "       variable_5  variable_6  variable_7  variable_8     ...       \\\n",
       "38735           0           0           0           0     ...        \n",
       "50451           0           0           0           0     ...        \n",
       "98518           0           0           0           0     ...        \n",
       "59451           0           0           0           0     ...        \n",
       "29756           0           0           0           0     ...        \n",
       "\n",
       "       variable_90  variable_91  variable_92  variable_93  variable_94  \\\n",
       "38735            0            0            0            0            0   \n",
       "50451            0            0            0            0            0   \n",
       "98518            0            0            0            0            0   \n",
       "59451            0            0            0            0            0   \n",
       "29756            0            0            0            0            0   \n",
       "\n",
       "       variable_95  variable_96  variable_97  variable_98  variable_99  \n",
       "38735            0            0            0            0            0  \n",
       "50451            0            0            0            0            0  \n",
       "98518            0            0            0            0            0  \n",
       "59451            0            0            0            0            0  \n",
       "29756            1            0            1            0            0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_=[]\n",
    "for c in range(0,count):\n",
    "    list_.append(0)\n",
    "for c in range(0,count):\n",
    "    list_.append(1)\n",
    "train=pd.DataFrame()\n",
    "train[\"Sentiment\"]=list_\n",
    "train=pd.concat([train,combine],axis=1)\n",
    "\n",
    "train = shuffle(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost\n",
    "Using Gradient boosting framework for training first for early stopping rounds and then training with full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n",
      "[0]\tval-rmse:0.499793\n",
      "Will train until val-rmse hasn't improved in 20 rounds.\n",
      "[20]\tval-rmse:0.495553\n",
      "[40]\tval-rmse:0.491975\n",
      "[60]\tval-rmse:0.488699\n",
      "[80]\tval-rmse:0.485955\n",
      "[100]\tval-rmse:0.483328\n",
      "[120]\tval-rmse:0.480956\n",
      "[140]\tval-rmse:0.478887\n",
      "[160]\tval-rmse:0.477023\n",
      "[180]\tval-rmse:0.475343\n",
      "[200]\tval-rmse:0.473853\n",
      "[220]\tval-rmse:0.472498\n",
      "[240]\tval-rmse:0.471258\n",
      "[260]\tval-rmse:0.470136\n",
      "[280]\tval-rmse:0.469108\n",
      "[300]\tval-rmse:0.468166\n",
      "[320]\tval-rmse:0.467275\n",
      "[340]\tval-rmse:0.466475\n",
      "[360]\tval-rmse:0.465741\n",
      "[380]\tval-rmse:0.465052\n",
      "[400]\tval-rmse:0.464426\n",
      "[420]\tval-rmse:0.46382\n",
      "[440]\tval-rmse:0.463256\n",
      "[460]\tval-rmse:0.462719\n",
      "[480]\tval-rmse:0.462219\n",
      "[500]\tval-rmse:0.461749\n",
      "[520]\tval-rmse:0.461294\n",
      "[540]\tval-rmse:0.460869\n",
      "[560]\tval-rmse:0.460483\n",
      "[580]\tval-rmse:0.460125\n",
      "[600]\tval-rmse:0.459773\n",
      "[620]\tval-rmse:0.45943\n",
      "[640]\tval-rmse:0.459128\n",
      "[660]\tval-rmse:0.458822\n",
      "[680]\tval-rmse:0.458543\n",
      "[700]\tval-rmse:0.458275\n",
      "[720]\tval-rmse:0.458016\n",
      "[740]\tval-rmse:0.457783\n",
      "[760]\tval-rmse:0.457547\n",
      "[780]\tval-rmse:0.457333\n",
      "[800]\tval-rmse:0.457141\n",
      "[820]\tval-rmse:0.456939\n",
      "[840]\tval-rmse:0.456757\n",
      "[860]\tval-rmse:0.456573\n",
      "[880]\tval-rmse:0.456416\n",
      "[900]\tval-rmse:0.456254\n",
      "[920]\tval-rmse:0.456102\n",
      "[940]\tval-rmse:0.455963\n",
      "[960]\tval-rmse:0.455819\n",
      "[980]\tval-rmse:0.455684\n",
      "[1000]\tval-rmse:0.455562\n",
      "[1020]\tval-rmse:0.455441\n",
      "[1040]\tval-rmse:0.455325\n",
      "[1060]\tval-rmse:0.455215\n",
      "[1080]\tval-rmse:0.455109\n",
      "[1100]\tval-rmse:0.455011\n",
      "[1120]\tval-rmse:0.454914\n",
      "[1140]\tval-rmse:0.454828\n",
      "[1160]\tval-rmse:0.45474\n",
      "[1180]\tval-rmse:0.454656\n",
      "[1200]\tval-rmse:0.45458\n",
      "[1220]\tval-rmse:0.454512\n",
      "[1240]\tval-rmse:0.454439\n",
      "[1260]\tval-rmse:0.45438\n",
      "[1280]\tval-rmse:0.454318\n",
      "[1300]\tval-rmse:0.45425\n",
      "[1320]\tval-rmse:0.454192\n",
      "[1340]\tval-rmse:0.45414\n",
      "[1360]\tval-rmse:0.454084\n",
      "[1380]\tval-rmse:0.454038\n",
      "[1400]\tval-rmse:0.453991\n",
      "[1420]\tval-rmse:0.453947\n",
      "[1440]\tval-rmse:0.453908\n",
      "[1460]\tval-rmse:0.453867\n",
      "[1480]\tval-rmse:0.453832\n",
      "[1500]\tval-rmse:0.4538\n",
      "[1520]\tval-rmse:0.453767\n",
      "[1540]\tval-rmse:0.453736\n",
      "[1560]\tval-rmse:0.453709\n",
      "[1580]\tval-rmse:0.453675\n",
      "[1600]\tval-rmse:0.45365\n",
      "[1620]\tval-rmse:0.453627\n",
      "[1640]\tval-rmse:0.453609\n",
      "[1660]\tval-rmse:0.453585\n",
      "[1680]\tval-rmse:0.453565\n",
      "[1700]\tval-rmse:0.453552\n",
      "[1720]\tval-rmse:0.453536\n",
      "[1740]\tval-rmse:0.453522\n",
      "[1760]\tval-rmse:0.453506\n",
      "[1780]\tval-rmse:0.453491\n",
      "[1800]\tval-rmse:0.453472\n",
      "[1820]\tval-rmse:0.453457\n",
      "[1840]\tval-rmse:0.453444\n",
      "[1860]\tval-rmse:0.45343\n",
      "[1880]\tval-rmse:0.453418\n",
      "[1900]\tval-rmse:0.453401\n",
      "[1920]\tval-rmse:0.453391\n",
      "[1940]\tval-rmse:0.453383\n",
      "[1960]\tval-rmse:0.453381\n",
      "[1980]\tval-rmse:0.453376\n",
      "[2000]\tval-rmse:0.45337\n",
      "[2020]\tval-rmse:0.453368\n",
      "[2040]\tval-rmse:0.453359\n",
      "[2060]\tval-rmse:0.453353\n",
      "[2080]\tval-rmse:0.453351\n",
      "[2100]\tval-rmse:0.453344\n",
      "[2120]\tval-rmse:0.453336\n",
      "[2140]\tval-rmse:0.453334\n",
      "[2160]\tval-rmse:0.453328\n",
      "[2180]\tval-rmse:0.453323\n",
      "[2200]\tval-rmse:0.453323\n",
      "Stopping. Best iteration:\n",
      "[2186]\tval-rmse:0.453322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = train[\"Sentiment\"]\n",
    "print(\"Training Started\")\n",
    "\n",
    "dtrain = xgb.DMatrix(train.drop('Sentiment', axis=1).iloc[0:int(count*0.8),:], y_train.iloc[0:int(count*0.8)])\n",
    "dtrain_all= xgb.DMatrix(train.drop('Sentiment', axis=1), y_train)\n",
    "dval = xgb.DMatrix(train.drop('Sentiment', axis=1).iloc[int(count*0.8):,:], y_train.iloc[int(count*0.8):])\n",
    "xgb_params = {\n",
    "    'eta': 0.005,\n",
    "    'max_depth': 12,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "partial_model = xgb.train(xgb_params, dtrain, num_boost_round=3000, evals=[(dval, 'val')],\n",
    "                      early_stopping_rounds=20, verbose_eval=20)\n",
    "num_boost_round = partial_model.best_iteration\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain_all, num_boost_round=num_boost_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract Sentiment of twitter Data and Class handling the input of data from tweepy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    temporary=text\n",
    "    text = text.map(desc_clean)\n",
    "    text = [[x for x in x.split() if x not in stop] for x in text]\n",
    "    text = [[stemmer.stem(x) for x in x] for x in text]\n",
    "    text = [[x for x in x if len(x) > 2] for x in text]\n",
    "    text = [' '.join(x) for x in text]\n",
    "    text = cv.fit_transform(text).todense()\n",
    "    combine_1 = pd.DataFrame(text)\n",
    "    combine_1.rename(columns= lambda x: 'variable_'+ str(x), inplace=True)\n",
    "    test=xgb.DMatrix(combine_1)\n",
    "    preds=model.predict(test)\n",
    "    y_pred=[]\n",
    "    for c in preds:\n",
    "        if(c>=0.5):\n",
    "            y_pred.append(\"Positive\")\n",
    "        else:\n",
    "            y_pred.append(\"Negative\")\n",
    "    i=0\n",
    "    while(i<len(temporary)):\n",
    "        print(temporary[i],\"   \",y_pred[i],\"  \",preds[i],\"\\n\\n\")\n",
    "        i+=1\n",
    "\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    tweet_number=0   # class variable\n",
    "\n",
    "    def __init__(self,max_tweets):\n",
    "        self.max_tweets=max_tweets # max number of tweets\n",
    "\n",
    "    def on_data(self, data):\n",
    "        self.tweet_number+=1   \n",
    "        try:\n",
    "            decoded = json.loads(data)\n",
    "            #print(decoded['text'].encode('latin1', 'ignore'))\n",
    "            A.append(decoded['text'].encode('latin1', 'ignore'))\n",
    "            #sentiment([decoded['text'].encode('latin1', 'ignore')])\n",
    "            #print(decoded['text'].encode('latin1', 'ignore'))\n",
    "            \n",
    "        except BaseException:\n",
    "            print('Error')\n",
    "            pass\n",
    "        except IncompleteRead:\n",
    "        # Oh well, reconnect and keep trucking\n",
    "            print('Error')\n",
    "            pass\n",
    "        except ProtocolError:\n",
    "        # Oh well, reconnect and keep trucking\n",
    "            print('Error')\n",
    "            pass\n",
    "        except KeyboardInterrupt:\n",
    "        # Or however you want to exit this loop\n",
    "            stream.disconnect()\n",
    "        if self.tweet_number>=self.max_tweets:\n",
    "            #sys.exit('Limit of '+str(self.max_tweets)+' tweets reached.')\n",
    "            if(len(A)>=self.max_tweets):\n",
    "                return False\n",
    "    def on_error(self, status):\n",
    "        print (\"Error \" + str(status))\n",
    "        if status == 420:\n",
    "            print(\"Rate Limited\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tweepy\n",
    "Get consumer key,consumer_key_secret,accces_token and accent_token_secret from twitter developer page.\n",
    "The values vary as :- 0 is negative and 1 is postive and intermediary values have mixed emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'@Josewittaph Hi I have a question'     Positive    0.750285 \n",
      "\n",
      "\n",
      "b'RT @syeddoha: Ro Nay San Lwin, if you are lecturing #Bangladesh, please leave the country as no one is interested in your message '     Positive    0.710377 \n",
      "\n",
      "\n",
      "b'I love you dad https://t.co/YEEPPW0E9N'     Positive    0.892878 \n",
      "\n",
      "\n",
      "b'@mindingthegaap Like, I\\'m pretty sure it\\'s like \"haha, look, a sausage pizza! We\\'re having a--\"\\n\\nNo.\\n\\nWHY MUST WE GENDER EVERYTHING'     Negative    0.229331 \n",
      "\n",
      "\n",
      "b'RT @hemlockspidey: one year and nine episodes later: \\n- went from an asshole to a mother of four \\n- nancy doesnt deserve him \\n- had o '     Positive    0.787309 \n",
      "\n",
      "\n",
      "b'When the teacher born and raised IE https://t.co/jijsLXqmpJ'     Positive    0.516939 \n",
      "\n",
      "\n",
      "b'RT @rallystarters: @exoticgamora @TheSWPrincess @debbiesideris @SpockResists @MrScottLads @NatCookResists @Alyssa_Milano @Havok_2017 '     Negative    0.166474 \n",
      "\n",
      "\n",
      "b'RT @ICHRI: #Iran\\'s Intelligence Ministry is \"inviting\" Sunni politicians to hotels and tell them to stop campaigning for Sunni '     Negative    0.445354 \n",
      "\n",
      "\n",
      "b'Love this combination of solid talent for this event wish I was attending now just to watch and listen! https://t.co/gHH9GFNmbq'     Positive    0.591501 \n",
      "\n",
      "\n",
      "b'@SteelersOilers @TheMarkPantano So all the conservatives including the former president fox news who committed sexu https://t.co/mX5AC2ydeq'     Negative    0.12274 \n",
      "\n",
      "\n",
      "b'DUDE SAME https://t.co/ZgtFswoeLd'     Positive    0.510111 \n",
      "\n",
      "\n",
      "b'And he speaks https://t.co/D2zpMu7vdU'     Positive    0.605948 \n",
      "\n",
      "\n",
      "b'@Chloe_GShore @MotelRocks Jesus Christ its up your whole Chloe. Its a disgrace. Even tho the top half is nice . W https://t.co/UmcoP6BYmD'     Positive    0.635713 \n",
      "\n",
      "\n",
      "b'Over 180 Women Say They Were Sexually Assaulted At A Popular Massage Franchise https://t.co/lDQDmjppxL via @Change'     Negative    0.380525 \n",
      "\n",
      "\n",
      "b'RT @AustinPriceless: The chants at the Tennessee basketball game tonight could be GOLD!'     Negative    0.197015 \n",
      "\n",
      "\n",
      "b'RT @vtvanhees: Exciting new job vacancy in our organisation on developing and innovating models for climate research! #escience '     Positive    0.597679 \n",
      "\n",
      "\n",
      "b'RT @SteveBannen: Why is it that Democrat leaders are bullying Rep. Conyers to resign while giving on-camera harasser Al Franken a pass, whi'     Negative    0.166987 \n",
      "\n",
      "\n",
      "b'RT @cReativi_D_: @itshannahvee Girl you got all the fine Alphas in your mentions with this one  https://t.co/oOFaPtma1T'     Positive    0.657594 \n",
      "\n",
      "\n",
      "b'Michael be nice to your mum! She is just worried about you#PeakyBlinders'     Positive    0.681785 \n",
      "\n",
      "\n",
      "b'RT @Giftedjawnie: Yall need a getaway with yall boo? Visit belamerr suites in Perryburg Ohio! https://t.co/O8qhbUYU3K'     Negative    0.444995 \n",
      "\n",
      "\n",
      "b'@_cingraham @NBCNewsPR Can they also address the amazing coincidence that they fired him right before the Variety story ran?'     Negative    0.350875 \n",
      "\n",
      "\n",
      "b'RT @generocity_: Funniest interview on The Breakfast Club  #NeverForget https://t.co/baB6OOPvQl'     Negative    0.450756 \n",
      "\n",
      "\n",
      "b'RT @h0t_p0ppy: The Australian government has described Japans decision to resume whaling in the southern ocean as deeply disappo '     Negative    0.136529 \n",
      "\n",
      "\n",
      "b\"RT @GlennKesslerWP: Jeez, everyone's a fact checker now -- even the Dutch embassy. https://t.co/zOecT8uxwf\"     Negative    0.143851 \n",
      "\n",
      "\n",
      "b'RT @BlvckGrip: the gender neutral term for sugar daddy is glucose guardian'     Positive    0.668242 \n",
      "\n",
      "\n",
      "b'RT @syeddoha: #Bangladesh This is what happens in events like this. It hurts, emotions fly - I recognize the work of Myanmar acti '     Negative    0.411261 \n",
      "\n",
      "\n",
      "b\"RT @LOLGOP: My favorite part of the Bible is when Jesus ran around destroying anyone who didn't celebrate his birthday with a very specific\"     Negative    0.0530204 \n",
      "\n",
      "\n",
      "b'40+ Best #Android #Apps of the Month [December 2017] https://t.co/KtGDyBBhSp https://t.co/bHHvGKyxpb'     Negative    0.348187 \n",
      "\n",
      "\n",
      "b\"RT @meatymcsorley: If she's lived 115+ years she can eat whatever she wants. https://t.co/unhD3Hx5gi\"     Negative    0.110002 \n",
      "\n",
      "\n",
      "b'RT @HarmlessYardDog: Wow Tim, You Really Make Me Think There is No Agenda Against White People.\\n\\nThanks for Putting My Mind at Ease!\\n\\nWi '     Positive    0.601107 \n",
      "\n",
      "\n",
      "b'@RSoniccc @Karmaholic_ @RafiCOD @ChemsoCOD @Gradicals_ @CammyMVP I still have proof of you boosting the howitzer fr https://t.co/topMTypizK'     Negative    0.184211 \n",
      "\n",
      "\n",
      "b'RT @jaivonnealexa: her and jaden are very intelligent and self aware but people try to mock or downplay them because they dont act th '     Negative    0.422258 \n",
      "\n",
      "\n",
      "b'RT @Dufined: Thank you Amazon. https://t.co/4RsOnUnPsh'     Negative    0.239898 \n",
      "\n",
      "\n",
      "b\"RT @WeebishVibe: I don't trust people who are organised enough to balance a ton of school work on top of life because you know who e \"     Negative    0.292598 \n",
      "\n",
      "\n",
      "b'RT @TheEllenShow: Youve never seen anything like this. BTS. Tomorrow. @BTS_twt #BTSxELLEN #BTSARMY https://t.co/tHeP02OZBy'     Negative    0.348789 \n",
      "\n",
      "\n",
      "b'I liked a @YouTube video https://t.co/AIbqUXquNT #BerasaBeneran Kangennya'     Positive    0.510893 \n",
      "\n",
      "\n",
      "b'RT @painefultruths: Check out this 1982 New York Times article about pedophile prostitution rings and how they are used to sell comprom '     Positive    0.882462 \n",
      "\n",
      "\n",
      "b'RT @Madik211: my cousins husband told me that boys will be boys until they find a girl they want to grow up for and ive never heard any'     Negative    0.295353 \n",
      "\n",
      "\n",
      "b'RT @AaliyahJay: Exactly  https://t.co/BiJYzb5MQR'     Negative    0.298837 \n",
      "\n",
      "\n",
      "b'RT @NextBestPicture: The 2017 Satellite Nominations Are Revealed- https://t.co/VYTfmvkLBN https://t.co/ZhMRdppTHE'     Negative    0.396956 \n",
      "\n",
      "\n",
      "b'RT @kylegriffin1: Trump says the GOP tax bill \"is going to cost me a fortune, believe me.\"\\n\\nNBC News found that Trump and his family '     Negative    0.314498 \n",
      "\n",
      "\n",
      "b'Eli to the jags  @RealMichaelKay @DonLagreca @Rosenbergradio'     Positive    0.573335 \n",
      "\n",
      "\n",
      "b'RT @karachikhatmal: Reminder that this statement comes in wake of genocidal extremists successfully holding the state hostage.  https://t.c'     Positive    0.797649 \n",
      "\n",
      "\n",
      "b\"RT @vanOnselenP: So let me get this straight - reactionaries demand a plebiscite, get one, don't like the result, demand any parliam \"     Negative    0.0640313 \n",
      "\n",
      "\n",
      "b\"RT @nigju_shania: Just cause a person is always positive and bubbly does not mean they don't go through struggles and hardships too. \"     Positive    0.522274 \n",
      "\n",
      "\n",
      "b'Could we talk more on twitter about the makeip? Id...  Um sure my dms are open https://t.co/OFqscSHMTx'     Negative    0.104253 \n",
      "\n",
      "\n",
      "b'RT @ahmedtwinkie: This man deserves an apology from the whole U.S smfh https://t.co/Vi6tWAJZuT'     Positive    0.67376 \n",
      "\n",
      "\n",
      "b'RT @makeupbyshaniah: Moonlight says hello https://t.co/6ljzFZsuii'     Negative    0.179852 \n",
      "\n",
      "\n",
      "b'RT @xForcades: My fat ass thought this was a chicken tender https://t.co/GF5nlUPvqK'     Positive    0.933433 \n",
      "\n",
      "\n",
      "b'@StarryTxt i think its the face.'     Negative    0.430021 \n",
      "\n",
      "\n",
      "b'RT @maryodonnell03: Me too. Lookin fwrd https://t.co/yKORMZwu0i'     Positive    0.677854 \n",
      "\n",
      "\n",
      "b'RT @Milly_Babyx333: ya b 19 n wanting babies n settling down. If u dont live a lil , dios'     Negative    0.435975 \n",
      "\n",
      "\n",
      "b'RT @vantaekim: imagine the door opening and you see these seven angels infront of you https://t.co/BjYCFDCyRM'     Negative    0.258154 \n",
      "\n",
      "\n",
      "b'RT @hourlyterrier: Yes, who can forget when Jeremy Corbyn first took charge, and the Labour moderates reached out a hand and said, \"le '     Positive    0.516189 \n",
      "\n",
      "\n",
      "b\"RT @NAMJOONPlC: Namjoon couldn't open the ambulance doors, a baby  https://t.co/W3vlW1NhZg\"     Positive    0.875242 \n",
      "\n",
      "\n",
      "b\"RT @chippznuff: can you really call yourself an ex-aid fan if you don't want to get into a bar fight with iijima\"     Negative    0.0853164 \n",
      "\n",
      "\n",
      "b\"DMX is coming for Mariah Carey's Christmas throne with a 'Rudolph the Red-Nosed Reindeer' cover https://t.co/X3h9BdMeR8\"     Negative    0.102898 \n",
      "\n",
      "\n",
      "b'Finished chibi/mini commissions done for @koileyy !!\\n( i love these designs i weep, thank you again for commissioni https://t.co/el2MJcQkjx'     Positive    0.693523 \n",
      "\n",
      "\n",
      "b'Chile these church ppl a mess'     Positive    0.589512 \n",
      "\n",
      "\n",
      "b'Store employees called for help after they heard him and could not unlock the safe. It took a locksmith and a... https://t.co/BxBshYQ70Y'     Positive    0.504745 \n",
      "\n",
      "\n",
      "b\"RT @hut34project: 'Information is the resolution of uncertainty' Claude E Shannon. #hut34project building an open distributed \"     Negative    0.178534 \n",
      "\n",
      "\n",
      "b'RT @DUALIPA: 1. Throw away your calendar\\n2. Break up with him so its one less gift\\n3. Marry the grinch https://t.co/HnlFt67eOI'     Negative    0.238693 \n",
      "\n",
      "\n",
      "b\"@mrsthinksalot @tots100 @2Hearts1Roof It's been lovely to talk to you tonight :) x #safeandtotal\"     Negative    0.282816 \n",
      "\n",
      "\n",
      "b'RT @psenews: Interesting that this exists to fight mental health in African American communities who suffer more from mental ill '     Negative    0.220586 \n",
      "\n",
      "\n",
      "b'RT @NK_Lemuel: 2017 and I am still unlearning a lot of things....2018 I am sure there is more to be unlearned.'     Negative    0.335703 \n",
      "\n",
      "\n",
      "b\"RT @VABVOX: #OnTheDayTrumpResigns Mike Pence will be POTUS.\\n\\nHe's the quiet ideological monster who's put women in jail for mis \"     Positive    0.801494 \n",
      "\n",
      "\n",
      "b'of course it is &gt;&gt;&gt; The Wheels Are Coming Off The Tax Bills Promised Deficit Trigger https://t.co/F6rFw5Waso via @TPM'     Negative    0.476268 \n",
      "\n",
      "\n",
      "b'RT @MatttttDent: Need the power of social media &amp; #SWFC family to help get this viral &amp; support my mini owl. He needs my support. WA '     Negative    0.435869 \n",
      "\n",
      "\n",
      "b'RT @miliondollameat: when u visit the pet adoption center https://t.co/XT0xqsqFE6'     Negative    0.185001 \n",
      "\n",
      "\n",
      "b'RT @Dolansupernova: GUYS BECAUSE I GOT 2/2 IMMA HELP U GET E AND Gs FOLLOW SO LIKE FOR GRAYSONS AND RT FOR ETHANS OR BOTH AND I WILL MA '     Positive    0.605292 \n",
      "\n",
      "\n",
      "b'@gripsnatch youre welcome'     Negative    0.491676 \n",
      "\n",
      "\n",
      "b\"@Wrighty_8 He's slow as a turtle man\"     Positive    0.573544 \n",
      "\n",
      "\n",
      "b'RT @NuNotebooks: To be in for the chance to #Win one of these bright Nu: Notebooks just FOLLOW our page and RT this post  '     Negative    0.121835 \n",
      "\n",
      "\n",
      "b'RT @UnboxTherapy: NEW VIDEO - Unlock Any MacBook Without The Password https://t.co/StEBULhTYN RT and share so others know to update! https'     Positive    0.606467 \n",
      "\n",
      "\n",
      "b'Grumors feel like they were before Y2K but they were literally just flying around last weekend. What a time to be a vol fan'     Negative    0.345842 \n",
      "\n",
      "\n",
      "b\"RT @thegreatkhaalid: i've done a lot of great things for a lot of ungrateful ppl\"     Negative    0.166618 \n",
      "\n",
      "\n",
      "b'RT @cp_salu: \"Nothings too broken to find a way back\"'     Positive    0.546732 \n",
      "\n",
      "\n",
      "b\"RT @spokenELLE: A room full of non-Black women and yet I didn't hear a pause https://t.co/tugrnqrUY1\"     Negative    0.181388 \n",
      "\n",
      "\n",
      "b'RT @marcsmalls: the entire twilight franchise really played my adolescent ass sooo dirty with this fake scene https://t.co/ajyYuNjYG7'     Negative    0.448538 \n",
      "\n",
      "\n",
      "b'RT @TheStagmania: Say, does anyone remember that time Matt Lauer moderated the Commander-In-Chief Forum and soft pedaled Trump while '     Positive    0.923917 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consumer_key=\"\"\n",
    "consumer_secret=\"\"\n",
    "\n",
    "access_token= \"\"\n",
    "access_token_secret= \"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "A=[]\n",
    "i=0\n",
    "while(i<4):\n",
    "    A=[]\n",
    "    l = StdOutListener(20)\n",
    "    stream = Stream(auth, l)\n",
    "    stream.filter(languages=[\"en\"],track=[\"a\", \"the\",\"you\"])\n",
    "    sentiment(pd.Series(A).astype(str))\n",
    "    time.sleep(40)\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
